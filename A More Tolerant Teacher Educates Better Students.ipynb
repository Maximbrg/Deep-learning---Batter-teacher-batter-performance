{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A More Tolerant Teacher Educates Better Students.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzCSXlTGsk-_","executionInfo":{"status":"ok","timestamp":1626859039917,"user_tz":-180,"elapsed":569754,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}},"outputId":"92cfda8f-34a9-4421-83cc-d8cc4bcdc9cf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLgxOQ-5UhzH","executionInfo":{"status":"ok","timestamp":1626859059404,"user_tz":-180,"elapsed":9368,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}},"outputId":"9c8eb58a-cfec-4cac-9a9f-16edb023d697"},"source":["!pip install git+https://github.com/darenr/scikit-optimize\n","!pip install Tab2Img"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/darenr/scikit-optimize\n","  Cloning https://github.com/darenr/scikit-optimize to /tmp/pip-req-build-hwlg_it7\n","  Running command git clone -q https://github.com/darenr/scikit-optimize /tmp/pip-req-build-hwlg_it7\n","Collecting pyaml\n","  Downloading pyaml-20.4.0-py2.py3-none-any.whl (17 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.19.5)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize==0.6+19.g180d6be) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.6+19.g180d6be) (1.0.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml->scikit-optimize==0.6+19.g180d6be) (3.13)\n","Building wheels for collected packages: scikit-optimize\n","  Building wheel for scikit-optimize (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-optimize: filename=scikit_optimize-0.6+19.g180d6be-py2.py3-none-any.whl size=75420 sha256=afa42e8dd918fd2db2cfec83d5df817b16cda4783e231096be869111a1b056cb\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5lhsufdn/wheels/11/4c/bf/aa91b51947ed5367f98d5a68c59afd45a275b85e9fc4827007\n","Successfully built scikit-optimize\n","Installing collected packages: pyaml, scikit-optimize\n","Successfully installed pyaml-20.4.0 scikit-optimize-0.6+19.g180d6be\n","Collecting Tab2Img\n","  Downloading tab2img-0.0.2-py3-none-any.whl (4.8 kB)\n","Installing collected packages: Tab2Img\n","Successfully installed Tab2Img-0.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8EFWMH8eq15d","executionInfo":{"status":"ok","timestamp":1626859061873,"user_tz":-180,"elapsed":2472,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["import pandas as pd\n","import time\n","from sklearn import metrics\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","from keras import backend as K\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras import applications as efn\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, GlobalAveragePooling2D, MaxPool2D, UpSampling2D,Lambda\n","from tensorflow.keras import optimizers\n","from keras.wrappers.scikit_learn import KerasClassifier\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","from keras.regularizers import l2\n","from keras.layers.advanced_activations import PReLU\n","from keras.layers.core import Dense, Dropout, Activation\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.datasets import fetch_covtype\n","from tab2img.converter import Tab2Img\n","import time\n","from sklearn.metrics import confusion_matrix as conf_m\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from tab2img.converter import Tab2Img\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","from tensorflow.keras import applications as efn\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, GlobalAveragePooling2D, MaxPool2D, UpSampling2D\n","from tensorflow.keras import optimizers\n","import pandas as pd\n","import tarfile\n","import scipy\n","import scipy.io\n","import matplotlib.pyplot as plt\n","import PIL\n","import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential, Model\n","from keras.preprocessing.image import ImageDataGenerator\n","import tensorflow as tf\n","import pickle\n","from PIL import Image\n","import numpy as np\n","\n","\n","data_folder_path = \"/content/drive/Shareddrives/ML-final project/data/\"\n","d1 = \"abalon.csv\"\n","d2 = \"annealing.csv\"\n","d3 = \"arrhythmia.csv\"\n","d4 = \"audiology-std.csv\"\n","d5 = \"autos.csv\"\n","d6 = \"balance-scale.csv\"\n","d7 = \"baseball.csv\"\n","d8 = \"car.csv\"\n","d9 = \"cardiotocography-3clases.csv\"\n","d10 = \"cardiotocography-10clases.csv\"\n","d11 = \"conn-bench-vowel-deterding.csv\"\n","d12 = \"contrac.csv\"\n","d13 = \"dermatology.csv\"\n","d14 = \"ecoli.csv\"\n","d15 = \"energy-y1.csv\"\n","d16 = \"energy-y2.csv\"\n","d17 = \"flags.csv\"\n","d18 = \"glass.csv\"\n","d19 = \"heart-va.csv\"\n","d20 = \"iris.csv\""],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jPktOnowsXw"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"AJNHi7ZSJoRF","executionInfo":{"status":"ok","timestamp":1626859092111,"user_tz":-180,"elapsed":22284,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["data_folder_path = \"/content/drive/Shareddrives/ML-final project/data/\"\n","data_names = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14,d15,d16,d17,d18,d19,d20]\n","data_frames = []\n","for csv_name in data_names:\n","  temp_df = pd.read_csv(data_folder_path + csv_name)\n","  temp_df = temp_df.rename(columns = {'clase': 'class', 'symboling': 'class', 'Hall_of_Fame': 'class'}, inplace = False)\n","  temp_df = temp_df.fillna(temp_df.mean())\n","  for col_name in temp_df.columns:\n","    if temp_df[col_name].dtype == \"object\":\n","      temp_df[col_name] = pd.Categorical(temp_df[col_name])\n","      temp_df[col_name] = temp_df[col_name].cat.codes\n","  X = temp_df.drop('class', axis=1)\n","  y = temp_df['class']\n","  data_frames.append((X,y,len(pd.unique(temp_df['class']))))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OdqFJh_wtGC6","executionInfo":{"status":"ok","timestamp":1626859092985,"user_tz":-180,"elapsed":3,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["def convert_to_array(X_train):\n","  new_x = []\n","  X_train_1 = np.array(X_train)\n","  for item in X_train_1:\n","    new_x.append(item)\n","  return  np.array(new_x)\n","  \n","\n","def indices_to_one_hot(data, nb_classes):\n","    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n","    targets = np.array(data).reshape(-1)\n","    return np.eye(nb_classes)[targets]\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8HVPp5CjIS1Q"},"source":["# Functions"]},{"cell_type":"markdown","metadata":{"id":"UM-pX12jInN5"},"source":["**Metrics**"]},{"cell_type":"code","metadata":{"id":"IqWm0RZlISFl","executionInfo":{"status":"ok","timestamp":1626859096937,"user_tz":-180,"elapsed":3955,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["def sensitivity(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    return true_positives / (possible_positives + K.epsilon())\n","\n","    # \n","\n","def specificity(y_true, y_pred):\n","    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n","    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n","    return true_negatives / (possible_negatives + K.epsilon())\n","\n","AUC_ROC = tf.keras.metrics.AUC(\n","    num_thresholds=200,\n","    curve=\"ROC\",\n","    summation_method=\"interpolation\",\n","    name=\"ROC\",\n","    dtype=None,\n","    thresholds=None,\n","    multi_label=False,\n","    num_labels=None,\n","    label_weights=None,\n","    from_logits=False,\n",")\n","\n","AUC_PR = tf.keras.metrics.AUC(\n","    num_thresholds=200,\n","    curve=\"PR\",\n","    summation_method=\"interpolation\",\n","    name=\"PR\",\n","    dtype=None,\n","    thresholds=None,\n","    multi_label=False,\n","    num_labels=None,\n","    label_weights=None,\n","    from_logits=False,\n",")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPvbEB9EIvQ5"},"source":["**Losses**"]},{"cell_type":"code","metadata":{"id":"h1JgKGKXIsLB","executionInfo":{"status":"ok","timestamp":1626859096938,"user_tz":-180,"elapsed":43,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["def weighted_loss(y_true, y_pred):\n","    return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n","\n","def get_weighted_loss(weights):\n","    def weighted_loss(y_true, y_pred):\n","        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n","    return weighted_loss\n","\n","\n","def calculating_class_weights(y_true):\n","    from sklearn.utils.class_weight import compute_class_weight\n","    number_dim = np.shape(y_true)[1]\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n","    return weights\n","\n","\n","def my_loss_fn_unbalnced(y_true, y_pred):\n","  cce = keras.losses.CategoricalCrossentropy()\n","  ro = 0.75\n","  sum = 0\n","  sum = tf.cast(sum, dtype='float32')\n","  r = tf.TensorArray(y_pred.dtype, 0, dynamic_size=True)\n","  for item in y_pred:\n","    result = tf.math.top_k(item, KK)\n","    value = result[0][0]\n","    i = 1\n","    mean = 0\n","    while i < KK:\n","       mean += result[0][i]\n","       i += 1\n","    mean = value - mean * (1 / (KK - 1))\n","    mean = tf.cast(mean, dtype='float32')\n","    sum += mean\n","\n","  return ro*cce(y_true, y_pred) + (((1-ro)*sum)  / BATCH_SIZE )"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"tRFn3nhyIhSH","executionInfo":{"status":"ok","timestamp":1626859096939,"user_tz":-180,"elapsed":37,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["def print_plots(history):\n","  #  \"Accuracy\"\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'validation'], loc='upper left')\n","  plt.show()\n","  # \"Loss\"\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'validation'], loc='upper left')\n","  plt.show()\n","\n","def eval_metrics(y_test, predictions):\n","  confusion_matrix = conf_m(y_test.argmax(axis=1), predictions.argmax(axis=1))\n","  # scores = distiller.evaluate([X_test,X_test], y_test, batch_size=15)\n","\n","  FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n","  FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n","  TP = np.diag(confusion_matrix)\n","  TN = confusion_matrix.sum() - (FP + FN + TP)\n","\n","  # Sensitivity, hit rate, recall, or true positive rate\n","  TPR = np.nan_to_num(TP/(TP+FN))\n","  # Specificity or true negative rate\n","  TNR = np.nan_to_num(TN/(TN+FP))\n","  # Precision or positive predictive value\n","  PPV = np.nan_to_num(TP/(TP+FP))\n","\n","  # Negative predictive value\n","  NPV = TN/(TN+FN)\n","  \n","  # Fall out or false positive rate\n","  FPR = FP/(FP+TN)\n","  # False negative rate\n","  FNR = FN/(TP+FN)\n","  # False discovery rate\n","  FDR = FP/(TP+FP)\n","\n","  # Overall accuracy\n","  ACC = np.nan_to_num((TP+TN)/(TP+FP+FN+TN))\n","\n","  AUC_ROC_m = AUC_ROC(y_test, predictions)\n","  AUC_PR_m = AUC_PR(y_test, predictions)\n","\n","  return np.average(ACC), np.average(TPR), np.average(TNR), np.average(PPV), float(AUC_ROC_m), float(AUC_PR_m)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hINdxIWezZop"},"source":["# Model by the article - A More Tolerant Teacher Educates Better Students"]},{"cell_type":"markdown","metadata":{"id":"1Bpxu32f-pz7"},"source":["Models"]},{"cell_type":"code","metadata":{"id":"HhmRC32HOikI","executionInfo":{"status":"ok","timestamp":1626859096940,"user_tz":-180,"elapsed":36,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["\n","def build_keras_base(hidden_layers = [500, 256, 64], dropout_rate = 0, \n","                     l2_penalty = 0.1, optimizer = 'adam',\n","                     n_class = 18, verbose = 1):\n","\n","    model=Sequential()\n","    for layers in hidden_layers:\n","      model.add(Dense(layers, activation='relu', kernel_regularizer = l2(l2_penalty)))  \n","      model.add(Dropout(dropout_rate))\n","      model.add(BatchNormalization())\n","\n","    model.add(Dense(n_class, activation='softmax'))\n","    \n","    if verbose == 1:\n","      loss = my_loss_fn_unbalnced\n","    else:\n","      loss = 'categorical_crossentropy'\n","    \n","    model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])   \n","    return model\n","\n","\n","class Distiller(keras.Model):\n","    def __init__(self, student, teacher):\n","        super(Distiller, self).__init__()\n","        self.teacher = teacher\n","        self.student = student\n","\n","    def call(self, x):\n","      return self.student(x)\n","\n","    def compile(\n","        self,\n","        optimizer,\n","        metrics,\n","        student_loss_fn,\n","        distillation_loss_fn,\n","        alpha=0.1,\n","        temperature=3,\n","    ):\n","        \"\"\" Configure the distiller.\n","\n","        Args:\n","            optimizer: Keras optimizer for the student weights\n","            metrics: Keras metrics for evaluation\n","            student_loss_fn: Loss function of difference between student\n","                predictions and ground-truth\n","            distillation_loss_fn: Loss function of difference between soft\n","                student predictions and soft teacher predictions\n","            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n","            temperature: Temperature for softening probability distributions.\n","                Larger temperature gives softer distributions.\n","        \"\"\"\n","        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n","        self.student_loss_fn = student_loss_fn\n","        self.distillation_loss_fn = distillation_loss_fn\n","        self.alpha = alpha\n","        self.temperature = temperature\n","\n","    def train_step(self, data):\n","        # Unpack data\n","        (x_1,x_2), y = data\n","\n","        # Forward pass of teacher\n","        teacher_predictions = self.teacher(x_1, training=False)\n","\n","        with tf.GradientTape() as tape:\n","            # Forward pass of student\n","            student_predictions = self.student(x_2, training=True)\n","\n","            # Compute losses\n","            student_loss = self.student_loss_fn(y, student_predictions)\n","            distillation_loss = self.distillation_loss_fn(\n","                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n","                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n","            )\n","            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n","\n","        # Compute gradients\n","        trainable_vars = self.student.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update the metrics configured in `compile()`.\n","        self.compiled_metrics.update_state(y, student_predictions)\n","\n","        # Return a dict of performance\n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update(\n","            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n","        )\n","        return results\n","\n","    def test_step(self, data):\n","        # Unpack the data\n","        (x_1,x_2), y = data\n","\n","        # Compute predictions\n","        y_prediction = self.student(x_2, training=False)\n","\n","        # Calculate the loss\n","        student_loss = self.student_loss_fn(y, y_prediction)\n","\n","        # Update the metrics.\n","        self.compiled_metrics.update_state(y, y_prediction)\n","\n","        # Return a dict of performance\n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update({\"student_loss\": student_loss})\n","        return results\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsWCOm3TJYgT"},"source":["\n","\n","num_folds_outter = 10\n","BATCH_SIZE = 32\n","\n","d = { 'Params':[], 'Accuracy':[],\t'TPR':[],\t'TNR':[],\t'Precision':[],\t'AUC':[], 'Curve':[], 'Time train':[], 'Time test':[]}\n","df_marks = pd.DataFrame(d)\n","\n","kfold_outter = KFold(n_splits=num_folds_outter, shuffle=True)\n","\n","for index, df in enumerate(data_frames):\n","  # if index == 19:\n","    print('============',index,'============')\n","    for train_valid, test in kfold_outter.split(df[0], df[1]):\n","\n","      num_of_class = df[2]\n","\n","      X_train = df[0].iloc[train_valid]\n","      y_train = df[1].iloc[train_valid]\n","      X_test = df[0].iloc[test]\n","      y_test = df[1].iloc[test]\n","\n","      X_train = convert_to_array(X_train)\n","      y_train = np.array(y_train)\n","      y_train = y_train.reshape(-1,1)\n","      y_train = indices_to_one_hot(y_train, num_of_class)\n","\n","      X_test = convert_to_array(X_test)\n","      y_test = np.array(y_test)\n","      y_test = y_test.reshape(-1,1)\n","      y_test = indices_to_one_hot(y_test, num_of_class)\n","\n","      model_keras = KerasClassifier(\n","      build_fn = build_keras_base,\n","      n_class = num_of_class,\n","      verbose = 0\n","  )\n","      class_weights = calculating_class_weights(y_train)\n","      earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n","\n","      callbacks = [earlystop]\n","\n","      dropout_rate_opts  = [0, 0.2, 0.5]\n","      hidden_layers_opts = [[500, 256], [1000, 500, 256], [100, 500]]\n","      l2_penalty_opts = [0, 0.01, 0.1, 0.5]\n","      keras_param_options = {\n","          'hidden_layers': hidden_layers_opts,\n","          'dropout_rate': dropout_rate_opts,  \n","          'l2_penalty': l2_penalty_opts\n","      }\n","\n","      rs_keras = RandomizedSearchCV( \n","        model_keras, \n","        param_distributions = keras_param_options,\n","        scoring = 'neg_log_loss',\n","        n_iter = 3, \n","        cv = 3,\n","        n_jobs = -1,\n","        verbose = 1\n","    )\n","      \n","      keras_fit_params = {   \n","        'callbacks': callbacks,\n","        'epochs': 20,\n","        'batch_size': 32,\n","        'validation_split':0.2,\n","        'verbose': 0\n","    }\n","      start = time.time()\n","\n","      num_folds_outter = 10\n","\n","      if num_of_class < 5:\n","        KK = 2\n","      elif num_of_class < 10:\n","        KK = 3\n","      else:\n","        KK = 4\n","\n","      rs_keras.fit(X_train, y_train,  **keras_fit_params)\n","\n","      teacher = rs_keras.best_estimator_.model\n","\n","      opt_params = []\n","      for param, value in rs_keras.best_params_.items():\n","          opt_params.append(value)\n","          # print('\\t{}: {}'.format(param, value))\n","\n","      generations = 4\n","\n","      for i in range(4):\n","\n","        student = build_keras_base(hidden_layers = opt_params[1], dropout_rate = opt_params[2], \n","                        l2_penalty = opt_params[0], optimizer = 'adam',\n","                        n_class = num_of_class, verbose = 0)\n","        \n","\n","        earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n","        # Initialize and compile distiller\n","\n","        distiller = Distiller(student=student, teacher=teacher)\n","        distiller.compile(\n","            optimizer=keras.optimizers.Adam(),\n","            metrics=['accuracy'],\n","            student_loss_fn=keras.losses.CategoricalCrossentropy(), # get_weighted_loss(class_weights)\n","            distillation_loss_fn=keras.losses.KLDivergence(),\n","            alpha=0.1,\n","            temperature=10,\n","        )\n","\n","        # Distill teacher to student\n","        distiller.fit([X_train,X_train], y_train, epochs=5, verbose=0)\n","\n","        teacher = distiller\n","\n","      # Evaluate student on test dataset\n","      end = time.time()\n","      training_time = (end - start)\n","      start = time.time()\n","      predictions = distiller.predict(X_test)\n","      end = time.time()\n","      confusion_matrix = conf_m(y_test.argmax(axis=1), predictions.argmax(axis=1))\n","      # scores = distiller.evaluate([X_test,X_test], y_test, batch_size=15)\n","\n","      FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n","      FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n","      TP = np.diag(confusion_matrix)\n","      TN = confusion_matrix.sum() - (FP + FN + TP)\n","\n","      # Sensitivity, hit rate, recall, or true positive rate\n","      TPR = TP/(TP+FN)\n","      # Specificity or true negative rate\n","      TNR = TN/(TN+FP) \n","      # Precision or positive predictive value\n","      PPV = np.nan_to_num(TP/(TP+FP))\n","\n","      # Negative predictive value\n","      NPV = TN/(TN+FN)\n","      \n","      # Fall out or false positive rate\n","      FPR = FP/(FP+TN)\n","      # False negative rate\n","      FNR = FN/(TP+FN)\n","      # False discovery rate\n","      FDR = FP/(TP+FP)\n","\n","      # Overall accuracy\n","      ACC = (TP+TN)/(TP+FP+FN+TN)\n","\n","      AUC_ROC_m = AUC_ROC(y_test, predictions)\n","      AUC_PR_m = AUC_PR(y_test, predictions)\n","\n","      new_row = {'Params':opt_params, 'Accuracy':np.average(ACC),\t'TPR':np.average(TPR),\t'TNR':np.average(TNR),\t'Precision':np.average(PPV),\t'AUC':float(AUC_ROC_m),\n","        'Curve':float(AUC_PR_m), 'Time train':training_time, 'Time test':end -start}\n","\n","\n","\n","      df_marks =  df_marks.append(new_row, ignore_index=True)\n","\n","    df_marks.to_csv(data_folder_path+'results_2.csv', sep='\\t', encoding='utf-8')\n","\n","\n","      \n","\n","\n","\n","df_marks.to_csv(data_folder_path+'results_2.csv', sep='\\t', encoding='utf-8')\n","  \n","df_marks"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YS_EEDRNUAcw"},"source":["# Improvment"]},{"cell_type":"markdown","metadata":{"id":"FobXuWh4UDKg"},"source":["**A Batter Teacher**"]},{"cell_type":"code","metadata":{"id":"l_ew_ufh3KoL","executionInfo":{"status":"ok","timestamp":1626859272751,"user_tz":-180,"elapsed":377,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["\n","def get_model(UP, SIZE, num_classes=2):\n","\n","\n","  base_model = efn.ResNet50(weights='imagenet', include_top=False, input_shape=(SIZE,SIZE,3))\n","\n","     \n","\n","  for layer in base_model.layers:\n","      if isinstance(layer, BatchNormalization):\n","          layer.trainable = True\n","      else:\n","          layer.trainable = False\n","\n","  model=Sequential()\n","  model.add(UpSampling2D(size=(UP, UP)))\n","\n","\n","  model.add(base_model)\n","  model.add(GlobalAveragePooling2D())\n","  model.add(Dense(500, activation='relu'))\n","  model.add(Dropout(.4))\n","  model.add(BatchNormalization())\n","\n","  model.add(BatchNormalization())\n","  model.add(Dense(num_classes, activation='softmax'))\n","\n","  model.compile(loss=my_loss_fn_unbalnced, optimizer='adam', metrics=['accuracy'])\n"," \n","\n","  return model"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RPY2lqPeUI9m"},"source":["**Experiments**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vggstSHQKexd","executionInfo":{"status":"ok","timestamp":1626859593963,"user_tz":-180,"elapsed":310789,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}},"outputId":"51540e55-9ccb-401e-e6d9-af0126449c7b"},"source":["\n","num_folds_outter = 10\n","d = {'GEN': [],'VAL':[], 'Accuracy':[],\t'TPR':[],\t'TNR':[],\t'Precision':[],\t'AUC':[], 'Curve':[], 'Time train':[], 'Time test':[]}\n","df_marks = pd.DataFrame(d)\n","BATCH_SIZE = 64\n","\n","kfold_outter = KFold(n_splits=num_folds_outter, shuffle=True)\n","for i in range(20):\n","  if i == 19:\n","    df = data_frames[i]\n","\n","    counter = 1\n","    for train_valid, test in kfold_outter.split(df[0], df[1]):\n","\n","        \n","          num_of_class = df[2]\n","\n","          n_class = num_of_class\n","\n","          train = df[0].values\n","          target = df[1].values\n","\n","          model = Tab2Img()\n","          images = model.fit_transform(train, target)\n","\n","          X_train_2 = train[train_valid]\n","          X_test_2 = train[test]\n","\n","          X_train = images[train_valid]\n","          y_train = df[1].iloc[train_valid]\n","          X_test = images[test]\n","          y_test = df[1].iloc[test]\n","\n","          X_train = convert_to_array(X_train)\n","          y_train = np.array(y_train)\n","          y_train = y_train.reshape(-1,1)\n","          y_train = indices_to_one_hot(y_train, num_of_class)\n","\n","          X_test = convert_to_array(X_test)\n","          y_test = np.array(y_test)\n","          y_test = y_test.reshape(-1,1)\n","          y_test = indices_to_one_hot(y_test, num_of_class)\n","\n","          X_train = np.repeat(X_train[..., np.newaxis], 3, -1)\n","          X_test = np.repeat(X_test[..., np.newaxis], 3, -1)\n","\n","\n","          if num_of_class < 5:\n","            KK = 2\n","          elif num_of_class < 10:\n","            KK = 3\n","          else:\n","            KK = 4\n","\n","          print('=======================================')\n","          if X_train.shape[1] == 6:\n","            model = get_model(6,36,num_of_class)\n","          elif X_train.shape[1] == 17:\n","            model = get_model(2,34,num_of_class)\n","          elif X_train.shape[1] == 8:\n","            model = get_model(4,32,num_of_class)\n","          elif X_train.shape[1] == 2:\n","            model = get_model(16,32,num_of_class)\n","          elif X_train.shape[1] == 4:\n","            model = get_model(8,32,num_of_class)\n","          elif X_train.shape[1] == 3:\n","            model = get_model(11,33,num_of_class)\n","          elif X_train.shape[1] == 5:\n","            model = get_model(7,35,num_of_class)\n","          \n","          if  X_train.shape[0] > 800:\n","            epoches_fit = 25\n","            batch_size_f = 64\n","          else:\n","            epoches_fit = 50\n","            batch_size_f = 32\n","\n","\n","\n","          start_1 = time.time()\n","          history = model.fit(\n","                              X_train,y_train,\n","                              batch_size = 64,\n","                              validation_split=0.2,\n","                              epochs = 1,\n","                              shuffle=True,\n","                              verbose = 1\n","\n","                              )\n","          end_1 = time.time()\n","          train_time = end_1 - start_1\n","\n","          start = time.time()\n","          predictions = model.predict(X_test)\n","          end = time.time()\n","\n","          ACC, TPR, TNR, PPV, AUC_ROC_m, AUC_PR_m = eval_metrics(y_test, predictions)\n","          \n","          new_row = {'GEN': 0,'VAL':counter, 'Accuracy':ACC,\t'TPR':TPR,\t'TNR':TNR,\t'Precision':PPV,\t'AUC':AUC_ROC_m,\n","            'Curve':AUC_PR_m, 'Time train':train_time, 'Time test':(end-start)}\n","\n","          # print(new_row)\n","\n","          generations = 2\n","\n","          teacher = model\n","\n","          for i in range(5):\n","            # start = time.time()\n","            student = build_keras_base(n_class = num_of_class,verbose = 0)\n","\n","\n","            earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n","            # Initialize and compile distiller\n","            # class_weights = calculating_class_weights(y_train)\n","\n","            distiller = Distiller(student=student, teacher=teacher)\n","            distiller.compile(\n","                optimizer=keras.optimizers.Adam(),\n","                metrics=['accuracy',\"Precision\",AUC_ROC,AUC_PR,sensitivity, specificity],\n","                student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=False),\n","                distillation_loss_fn=keras.losses.KLDivergence(),\n","                alpha=0.1,\n","                temperature=10,\n","            )\n","\n","            # Distill teacher to student\n","            start_1 = time.time()\n","            distiller.fit([X_train,X_train_2], y_train, epochs=epoches_fit, verbose=0, batch_size = batch_size_f)\n","            end_1 = time.time()\n","\n","            train_time += (end_1 - start_1)\n","\n","            start = time.time()\n","            predictions = distiller.predict(X_test_2)\n","            end = time.time()\n","\n","            ACC, TPR, TNR, PPV, AUC_ROC_m, AUC_PR_m = eval_metrics(y_test, predictions)\n","\n","            if new_row['Curve'] < AUC_PR_m:\n","\n","              new_row = {'GEN': i+1,'VAL':counter, 'Accuracy':ACC,\t'TPR':TPR,\t'TNR':TNR, 'Precision':PPV,\t'AUC':AUC_ROC_m,\n","                'Curve':AUC_PR_m, 'Time train':train_time, 'Time test':end-start}\n","\n","            teacher = distiller\n","            X_test = X_test_2 \n","            X_train = X_train_2 \n","            \n","          counter += 1\n","            \n","          df_marks =  df_marks.append(new_row, ignore_index=True)\n","\n","    df_marks.to_csv(data_folder_path+'results_3.csv', sep='\\t', encoding='utf-8')\n","      \n","      \n","\n","df_marks.head()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["=======================================\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 2s 0us/step\n","2/2 [==============================] - 20s 1s/step - loss: 0.9678 - accuracy: 0.5093 - val_loss: 1.0789 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in true_divide\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in true_divide\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n"],"name":"stderr"},{"output_type":"stream","text":["+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","1/2 [==============>...............] - ETA: 4s - loss: 1.2821 - accuracy: 0.2969WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x7f61329eae60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","2/2 [==============================] - 6s 1s/step - loss: 1.0234 - accuracy: 0.4537 - val_loss: 1.7488 - val_accuracy: 0.0000e+00\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","1/2 [==============>...............] - ETA: 4s - loss: 1.1530 - accuracy: 0.3594WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f632a7284d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","2/2 [==============================] - 6s 987ms/step - loss: 0.9451 - accuracy: 0.4815 - val_loss: 0.2536 - val_accuracy: 0.9630\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","2/2 [==============================] - 6s 1s/step - loss: 1.1669 - accuracy: 0.3333 - val_loss: 0.7300 - val_accuracy: 0.5926\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","2/2 [==============================] - 6s 963ms/step - loss: 1.0368 - accuracy: 0.4537 - val_loss: 1.3650 - val_accuracy: 0.0000e+00\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","2/2 [==============================] - 6s 1s/step - loss: 1.1470 - accuracy: 0.4537 - val_loss: 0.1968 - val_accuracy: 1.0000\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","2/2 [==============================] - 6s 978ms/step - loss: 1.1570 - accuracy: 0.3889 - val_loss: 1.3575 - val_accuracy: 0.0000e+00\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","2/2 [==============================] - 6s 1s/step - loss: 1.0229 - accuracy: 0.4537 - val_loss: 0.7384 - val_accuracy: 0.0370\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","2/2 [==============================] - 6s 958ms/step - loss: 1.2537 - accuracy: 0.3611 - val_loss: 0.8395 - val_accuracy: 0.1111\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n","=======================================\n","2/2 [==============================] - 6s 981ms/step - loss: 1.0126 - accuracy: 0.4352 - val_loss: 3.1262 - val_accuracy: 0.0000e+00\n","+++++++++++++++++++++++++++++++++\n","+++++++++++++++++++++++++++++++++\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>GEN</th>\n","      <th>VAL</th>\n","      <th>Accuracy</th>\n","      <th>TPR</th>\n","      <th>TNR</th>\n","      <th>Precision</th>\n","      <th>AUC</th>\n","      <th>Curve</th>\n","      <th>Time train</th>\n","      <th>Time test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.999833</td>\n","      <td>0.999667</td>\n","      <td>37.728694</td>\n","      <td>0.101182</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>0.911111</td>\n","      <td>0.916667</td>\n","      <td>0.939394</td>\n","      <td>0.888889</td>\n","      <td>0.998845</td>\n","      <td>0.997730</td>\n","      <td>22.103323</td>\n","      <td>0.099791</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>0.955556</td>\n","      <td>0.916667</td>\n","      <td>0.962963</td>\n","      <td>0.952381</td>\n","      <td>0.999956</td>\n","      <td>0.999912</td>\n","      <td>23.371987</td>\n","      <td>0.103387</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.999622</td>\n","      <td>0.999259</td>\n","      <td>17.113719</td>\n","      <td>0.100307</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.999378</td>\n","      <td>0.998807</td>\n","      <td>22.642606</td>\n","      <td>0.095472</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   GEN  VAL  Accuracy       TPR  ...       AUC     Curve  Time train  Time test\n","0  4.0  1.0  1.000000  1.000000  ...  0.999833  0.999667   37.728694   0.101182\n","1  4.0  2.0  0.911111  0.916667  ...  0.998845  0.997730   22.103323   0.099791\n","2  4.0  3.0  0.955556  0.916667  ...  0.999956  0.999912   23.371987   0.103387\n","3  2.0  4.0  1.000000  1.000000  ...  0.999622  0.999259   17.113719   0.100307\n","4  4.0  5.0  1.000000  1.000000  ...  0.999378  0.998807   22.642606   0.095472\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"8VaRMJVyOPPU","executionInfo":{"status":"ok","timestamp":1626859893108,"user_tz":-180,"elapsed":381,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}},"outputId":"2d72121c-e51f-45ae-ea5c-d73e549893a0"},"source":["\n","df_marks.head()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>GEN</th>\n","      <th>VAL</th>\n","      <th>Accuracy</th>\n","      <th>TPR</th>\n","      <th>TNR</th>\n","      <th>Precision</th>\n","      <th>AUC</th>\n","      <th>Curve</th>\n","      <th>Time train</th>\n","      <th>Time test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.999833</td>\n","      <td>0.999667</td>\n","      <td>37.728694</td>\n","      <td>0.101182</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>0.911111</td>\n","      <td>0.916667</td>\n","      <td>0.939394</td>\n","      <td>0.888889</td>\n","      <td>0.998845</td>\n","      <td>0.997730</td>\n","      <td>22.103323</td>\n","      <td>0.099791</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>0.955556</td>\n","      <td>0.916667</td>\n","      <td>0.962963</td>\n","      <td>0.952381</td>\n","      <td>0.999956</td>\n","      <td>0.999912</td>\n","      <td>23.371987</td>\n","      <td>0.103387</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.999622</td>\n","      <td>0.999259</td>\n","      <td>17.113719</td>\n","      <td>0.100307</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.999378</td>\n","      <td>0.998807</td>\n","      <td>22.642606</td>\n","      <td>0.095472</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   GEN  VAL  Accuracy       TPR  ...       AUC     Curve  Time train  Time test\n","0  4.0  1.0  1.000000  1.000000  ...  0.999833  0.999667   37.728694   0.101182\n","1  4.0  2.0  0.911111  0.916667  ...  0.998845  0.997730   22.103323   0.099791\n","2  4.0  3.0  0.955556  0.916667  ...  0.999956  0.999912   23.371987   0.103387\n","3  2.0  4.0  1.000000  1.000000  ...  0.999622  0.999259   17.113719   0.100307\n","4  4.0  5.0  1.000000  1.000000  ...  0.999378  0.998807   22.642606   0.095472\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"H8_9BH0fciec","executionInfo":{"status":"ok","timestamp":1626869329808,"user_tz":-180,"elapsed":81031,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}}},"source":["import numpy as np\n","import pandas as pd\n","\n","df_time = pd.DataFrame({'time': []})\n","\n","features = [ 3,31, 262, 159, 25, 4, 16, 6, 21, 21, 11, 9, 34, 7, 8, 8, 28, 9, 12, 4]\n","\n","for feature in features:\n","\n","  for i in range(10):\n","    df = pd.DataFrame(np.random.uniform(0,1,size=(1000, 31)))\n","    df_y = pd.DataFrame(np.random.randint(0,3,size=(1000, 1)))\n","\n","    X_train = convert_to_array(df)\n","\n","    model = Tab2Img()\n","    X_train = model.fit_transform(X_train, df_y.values)\n","\n","    model = build_keras_base()\n","\n","    # if X_train.shape[1] == 6:\n","    #   model = get_model(6,36,num_of_class)\n","    # elif X_train.shape[1] == 17:\n","    #   model = get_model(2,34,num_of_class)\n","    # elif X_train.shape[1] == 8:\n","    #   model = get_model(4,32,num_of_class)\n","    # elif X_train.shape[1] == 2:\n","    #   model = get_model(16,32,num_of_class)\n","    # elif X_train.shape[1] == 4:\n","    #   model = get_model(8,32,num_of_class)\n","    # elif X_train.shape[1] == 3:\n","    #   model = get_model(11,33,num_of_class)\n","    # elif X_train.shape[1] == 5:\n","    #   model = get_model(7,35,num_of_class)\n","\n","    X_train = np.repeat(X_train[..., np.newaxis], 3, -1)\n","\n","    start = time.time()\n","    predict = model.predict(X_train)\n","    end = time.time()\n","    row = {'time': end-start}\n","    df_time =  df_time.append(row, ignore_index=True)\n","\n","df_time.to_csv(data_folder_path+'results_4.csv', sep='\\t', encoding='utf-8')"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"jrrJMXnZsv1w","executionInfo":{"status":"ok","timestamp":1626865645562,"user_tz":-180,"elapsed":8,"user":{"displayName":"Max Bragilovski","photoUrl":"","userId":"13234433401056472289"}},"outputId":"96cd1ba6-a820-4e46-99ef-f590a7cc11e9"},"source":["df_time\n"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.933589</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.950438</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.003186</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.041372</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.076138</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2.032463</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.542524</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2.060372</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2.041564</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.945109</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       time\n","0  1.933589\n","1  1.950438\n","2  2.003186\n","3  2.041372\n","4  2.076138\n","5  2.032463\n","6  2.542524\n","7  2.060372\n","8  2.041564\n","9  1.945109"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"TAJ8MSJOUYn0"},"source":["# Statisticl tests - friedman's"]},{"cell_type":"code","metadata":{"id":"o_4JJtrTOwvO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626703939437,"user_tz":-180,"elapsed":385,"user":{"displayName":"Omer Reichstein","photoUrl":"","userId":"04221663958064353311"}},"outputId":"069b5924-d7ae-495f-f560-255805ef9a78"},"source":["#statisticl tests - friedman's\n","\n","# !pip install scikit_posthocs\n","import pandas as pd\n","from scipy.stats import friedmanchisquare\n","import scikit_posthocs as sp\n","\n","avg_scores_base = [0.575355089,0.605236263,0.617489813,0.610611191,0.608549949,0.621841123,0.656721626,0.70341512,0.741710655,0.754682287\n",",0.760062099,0.745912285,0.733846726,0.736557243,0.741724752,0.748451151,0.749622821,0.747659547,0.743561954,0.741126907]\n","\n","avg_scores_teacher = [0.7598,0.7968,0.3492,0.6384,0.2861,0.958,0.8447,0.9744,0.9624,0.8672,0.8385,0.5288,0.9923,0.7943,0.9318,0.9355,0.456,0.609,0.3514,0.9186]\n","\n","avg_scores_improvement = [0.7577053,0.9572435,0.7112947,0.8842785,0.5749621,0.853707,0.983879,0.9998219,0.9818908,0.9268775,0.9981706,0.5762453,0.9932229,0.9168473,0.9534546\n",",0.9013419,0.5699038,0.7665592,0.3949459,0.9974408]\n","\n","avg = []\n","for i in range(20):\n","  avg.append((avg_scores_base[i],avg_scores_teacher[i],avg_scores_improvement[i]))\n","stat, p = friedmanchisquare(*avg)\n","print('Statistics=%.10f, p=%.10f' % (stat, p))\n","\n","names = [\"base\",\"teacher\", \"improved\"]\n","\n","hoc_test=sp.posthoc_nemenyi_friedman(avg)\n","col_name={}\n","for name in names:\n","    col_name.update({names.index(name):name})\n","hoc_test.rename(columns=col_name,index=col_name,inplace = True)\n","print(hoc_test)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Statistics=30.6380952381, p=0.0442186293\n","             base  teacher  improved\n","base      1.00000  0.51037   0.00100\n","teacher   0.51037  1.00000   0.03067\n","improved  0.00100  0.03067   1.00000\n"],"name":"stdout"}]}]}